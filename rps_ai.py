#!/usr/bin/env python3
"""
advanced_rps_ai.py

Advanced Rock-Paper-Scissors AI:
 - Rule-based predictors: Frequency, Markov (order=2), Pattern
 - Neural predictor (MLP) that is trained by self-play + curriculum
 - Ensemble decision: adaptive weights based on validation accuracy
 - If model file does not exist, trainer will run (configurable, e.g. 10_000_000 simulations)

Author: generated by Mohammad Taha Gorji 
"""

import argparse
import random
import math
import os
import time
from collections import defaultdict, deque, Counter
from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict

# Optional imports
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
except Exception as e:
    raise RuntimeError("This script requires PyTorch. Install with `pip install torch`.") from e

try:
    from tqdm import trange, tqdm
except Exception:
    def trange(x, **kwargs):
        return range(x)
    def tqdm(x, **kwargs):
        return x

# ---------- Constants & helpers ----------
MOVES = ["rock", "paper", "scissors"]
MOVE_TO_IDX = {m: i for i, m in enumerate(MOVES)}
IDX_TO_MOVE = {i: m for m, i in MOVE_TO_IDX.items()}

BEATS = {"rock": "scissors", "scissors": "paper", "paper": "rock"}
COUNTER = {m: next_m for next_m, m in BEATS.items()}  # move that beats the key

def winner(move_a: str, move_b: str) -> int:
    """Return 1 if a wins, -1 if b wins, 0 draw"""
    if move_a == move_b:
        return 0
    if BEATS[move_a] == move_b:
        return 1
    return -1

def one_hot_move(idx: int) -> List[int]:
    v = [0]*3
    v[idx] = 1
    return v

def encode_history(history: List[int], history_len: int) -> List[float]:
    """Encode last `history_len` moves as flattened one-hot vector.
       If history shorter, pad with uniform 1/3 distribution (neutral).
    """
    vec = []
    padding = max(0, history_len - len(history))
    for _ in range(padding):
        vec.extend([1/3, 1/3, 1/3])
    # take only the last `history_len` moves
    for idx in history[-history_len:]:
        vec.extend(one_hot_move(idx))
    return vec

# ---------- Rule-based predictors ----------
class FrequencyPredictor:
    def __init__(self):
        self.counts = [0,0,0]
    def update(self, move_idx):
        self.counts[move_idx] += 1
    def predict(self) -> Optional[int]:
        s = sum(self.counts)
        if s == 0:
            return None
        return int(max(range(3), key=lambda i: self.counts[i]))

class MarkovPredictor:
    # order=2 Markov chain
    def __init__(self, order=2):
        self.order = order
        self.transitions = {}  # key: tuple(prev...) -> Counter of next moves
        self.history = []
    def update(self, move_idx):
        if len(self.history) >= self.order:
            key = tuple(self.history[-self.order:])
            self.transitions.setdefault(key, [0,0,0])
            self.transitions[key][move_idx] += 1
        self.history.append(move_idx)
    def predict(self) -> Optional[int]:
        if len(self.history) < self.order:
            return None
        key = tuple(self.history[-self.order:])
        if key not in self.transitions:
            return None
        counts = self.transitions[key]
        if sum(counts)==0:
            return None
        return int(max(range(3), key=lambda i: counts[i]))

class PatternPredictor:
    # Search for repeated suffix patterns and return most common following move
    def __init__(self, max_k=6):
        self.max_k = max_k
        self.history = []
    def update(self, move_idx):
        self.history.append(move_idx)
    def predict(self) -> Optional[int]:
        n = len(self.history)
        if n < 2:
            return None
        for k in range(min(self.max_k, n-1), 0, -1):
            pattern = tuple(self.history[-k:])
            follows = [0,0,0]
            for i in range(n - k):
                if tuple(self.history[i:i+k]) == pattern:
                    if i + k < n:
                        follows[self.history[i+k]] += 1
            if sum(follows) > 0:
                return int(max(range(3), key=lambda i: follows[i]))
        return None

# ---------- Neural predictor ----------
class NeuralPredictor(nn.Module):
    def __init__(self, history_len:int=8, hidden_sizes=[128,64]):
        super().__init__()
        self.history_len = history_len
        input_size = history_len * 3
        layers = []
        last = input_size
        for h in hidden_sizes:
            layers.append(nn.Linear(last, h))
            layers.append(nn.ReLU())
            last = h
        layers.append(nn.Linear(last, 3))  # logits for 3 moves
        self.net = nn.Sequential(*layers)
    def forward(self, x):
        return self.net(x)  # returns logits

# ---------- Self-play trainer ----------
@dataclass
class TrainerConfig:
    history_len: int = 8
    batch_size: int = 4096
    lr: float = 1e-3
    epochs_per_save: int = 1
    selfplay_steps: int = 100_000    # set to large number for heavy training
    curriculum_mix: float = 0.5      # probability to play vs rule-based opponent
    eval_every: int = 10_000
    eval_games: int = 2000
    model_path: str = "rps_predictor.pt"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

class SelfPlayTrainer:
    def __init__(self, cfg: TrainerConfig):
        self.cfg = cfg
        self.device = torch.device(cfg.device)
        self.model = NeuralPredictor(history_len=cfg.history_len).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=cfg.lr)
        self.criterion = nn.CrossEntropyLoss()
        # Curriculum opponents
        self.freq_op = FrequencyPredictor()
        self.markov_op = MarkovPredictor(order=2)
        self.pattern_op = PatternPredictor(max_k=6)
        # keep validation accuracies as moving averages
        self.val_accs = {"freq": 0.0, "markov": 0.0, "pattern": 0.0}

    def sample_opponent(self):
        # randomly choose a rule-based opponent to play vs
        return random.choice(["freq", "markov", "pattern"])

    def simulate_batch(self, batch_size: int, use_model_vs_model: bool=False):
        """
        Simulate `batch_size` simultaneous one-step plays, returning:
        - states: tensor [batch_size, history_len*3]
        - labels: tensor [batch_size] (opponent move idx)
        The opponent will be either a rule-based agent (curriculum) or model (self-play).
        For speed we maintain per-sample history deques.
        """
        hlen = self.cfg.history_len
        states = []
        labels = []
        # maintain histories per simulation instance
        histories = [deque(maxlen=hlen) for _ in range(batch_size)]
        # We'll simulate few warm-up rounds to fill history randomly
        warmup = max(0, hlen - 2)
        for _ in range(warmup):
            for h in histories:
                h.append(random.randrange(3))
        # For each instance, decide whether opponent is rule-based or model
        # We'll mix opponents across the batch
        for i in range(batch_size):
            if random.random() < self.cfg.curriculum_mix:
                opp_type = self.sample_opponent()
                # create local short history object for that opponent so rule-based can use it
                # For rule-based predictions we only need the latest moves stored in internal predictors.
                # To get a label, we sample an opponent move according to simple heuristics:
                if opp_type == "freq":
                    # sample according to counts so far (but in batch we don't have global counts) -> just sample random with bias
                    label = random.choices(range(3), weights=[1,1,1])[0]
                elif opp_type == "markov":
                    label = random.randrange(3)
                else:
                    label = random.randrange(3)
                # But to ensure richer signal, choose label with mild bias to pattern of last item
                if len(histories[i])>0 and random.random() < 0.35:
                    # bias toward repeating last move
                    label = histories[i][-1]
                labels.append(label)
                states.append(encode_history(list(histories[i]), hlen))
                # update history for that instance
                histories[i].append(label)
            else:
                # self-play vs model: draw opponent move from model stochastically
                state_vec = encode_history(list(histories[i]), hlen)
                states.append(state_vec)
                # use model to sample move
                with torch.no_grad():
                    x = torch.tensor([state_vec], dtype=torch.float32, device=self.device)
                    logits = self.model(x)
                    probs = torch.softmax(logits, dim=-1).cpu().numpy().ravel()
                    label = int(random.choices([0,1,2], weights=probs, k=1)[0])
                labels.append(label)
                histories[i].append(label)
        states_t = torch.tensor(states, dtype=torch.float32, device=self.device)
        labels_t = torch.tensor(labels, dtype=torch.long, device=self.device)
        return states_t, labels_t

    def train(self, total_steps: int):
        cfg = self.cfg
        steps = 0
        best_val = -1.0
        # moving average baseline for loss if needed
        while steps < total_steps:
            to_do = min(cfg.batch_size, total_steps - steps)
            states, labels = self.simulate_batch(cfg.batch_size)
            logits = self.model(states)
            loss = self.criterion(logits, labels)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            steps += cfg.batch_size

            # periodic evaluation & logging
            if steps % cfg.eval_every == 0 or steps >= total_steps:
                val_acc = self.evaluate(cfg.eval_games)
                print(f"[train] step={steps}/{total_steps}  loss={loss.item():.5f}  val_acc={val_acc*100:.2f}%")
                # save best
                if val_acc > best_val:
                    best_val = val_acc
                    torch.save({
                        'model_state': self.model.state_dict(),
                        'cfg': vars(cfg)
                    }, cfg.model_path)
                    print(f"[train] saved best model with val_acc={best_val*100:.2f}% -> {cfg.model_path}")
        # final save
        torch.save({
            'model_state': self.model.state_dict(),
            'cfg': vars(cfg)
        }, cfg.model_path)
        print("[train] finished training and saved model.")

    def evaluate(self, n_games: int=2000) -> float:
        """
        Evaluate the predictor by measuring prediction accuracy vs rule-based opponents.
        Returns average accuracy across three rules.
        """
        accs = []
        for name in ["freq", "markov", "pattern"]:
            correct = 0
            total = 0
            # simulate short games where opponent is that rule-based agent
            # reset rule-based
            freq_b = FrequencyPredictor()
            markov_b = MarkovPredictor(order=2)
            pattern_b = PatternPredictor(max_k=6)
            for _ in range(n_games):
                # generate a context history by sampling random moves
                history = [random.randrange(3) for _ in range(self.cfg.history_len)]
                state_vec = torch.tensor([encode_history(history, self.cfg.history_len)], dtype=torch.float32, device=self.device)
                with torch.no_grad():
                    logits = self.model(state_vec)
                    pred_idx = int(torch.argmax(logits, dim=-1).cpu().numpy()[0])
                # generate the true opponent move according to the rule
                if name == "freq":
                    # simulate freq opponent with slight bias toward history distribution
                    counts = Counter(history)
                    weights = [counts[i]+1 for i in range(3)]
                    true = random.choices([0,1,2], weights=weights, k=1)[0]
                elif name == "markov":
                    # assume last two of history
                    if len(history) >= 2:
                        key = tuple(history[-2:])
                        # choose next either repeating last or random
                        if random.random() < 0.5:
                            true = history[-1]
                        else:
                            true = random.randrange(3)
                    else:
                        true = random.randrange(3)
                else:
                    # pattern: repeat suffix with some prob
                    if random.random() < 0.4:
                        true = history[-1]
                    else:
                        true = random.randrange(3)
                if pred_idx == true:
                    correct += 1
                total += 1
            accs.append(correct / total if total>0 else 0.0)
        avg = sum(accs)/len(accs)
        # update moving val accs for ensemble weighting if desired
        self.val_accs = {"freq": accs[0], "markov": accs[1], "pattern": accs[2]}
        return avg

# ---------- Ensemble AI for interactive play ----------
class EnsembleRPSAI:
    def __init__(self, history_len=8, model_path="rps_predictor.pt", device="cpu"):
        self.history_len = history_len
        self.device = torch.device(device)
        self.freq = FrequencyPredictor()
        self.markov = MarkovPredictor(order=2)
        self.pattern = PatternPredictor(max_k=6)
        # load model if exists
        self.model = None
        if os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                m = NeuralPredictor(history_len=history_len).to(self.device)
                m.load_state_dict(checkpoint['model_state'])
                m.eval()
                self.model = m
                print(f"[ensemble] Loaded neural predictor from {model_path}")
            except Exception as e:
                print("[ensemble] Failed to load model:", e)
                self.model = None
        else:
            print("[ensemble] No neural predictor found at", model_path)

        # stats for adaptive weighting
        self.pred_stats = {
            "freq": {"correct":0, "total":0},
            "markov": {"correct":0, "total":0},
            "pattern": {"correct":0, "total":0},
            "neural": {"correct":0, "total":0}
        }
        self.history = []  # opponent history as ints

    def update_rule_predictors(self, move_idx):
        self.freq.update(move_idx)
        self.markov.update(move_idx)
        self.pattern.update(move_idx)
        self.history.append(move_idx)

    def predict_each(self) -> Dict[str, Optional[int]]:
        """Return predictions from each predictor (move indices or None)"""
        preds = {}
        preds['freq'] = self.freq.predict()
        preds['markov'] = self.markov.predict()
        preds['pattern'] = self.pattern.predict()
        if self.model is not None:
            state_vec = encode_history(self.history, self.history_len)
            x = torch.tensor([state_vec], dtype=torch.float32, device=self.device)
            with torch.no_grad():
                logits = self.model(x)
                pred_idx = int(torch.argmax(logits, dim=-1).cpu().numpy()[0])
            preds['neural'] = pred_idx
        else:
            preds['neural'] = None
        return preds

    def choose_move(self, exploration=0.03) -> Tuple[str, dict]:
        """
        Choose AI move:
         - gather predictions from each predictor
         - compute weights proportional to historical accuracy (with small floor)
         - weighted voting -> predicted opponent move -> counter
         - apply small exploration probability
        Returns (ai_move_str, metadata)
        """
        if random.random() < exploration:
            return random.choice(MOVES), {"reason":"explore"}

        preds = self.predict_each()
        # compute weights
        def acc_stat(name):
            t = max(1, self.pred_stats[name]['total'])
            return self.pred_stats[name]['correct'] / t
        weights = {
            "freq": 0.05 + acc_stat('freq'),
            "markov": 0.05 + acc_stat('markov'),
            "pattern": 0.05 + acc_stat('pattern'),
            "neural": 0.05 + acc_stat('neural') if preds.get('neural') is not None else 0.0
        }
        # votes
        votes = [0.0, 0.0, 0.0]
        for name, p in preds.items():
            if p is None:
                continue
            votes[p] += weights.get(name, 0.05)
        if all(v == 0 for v in votes):
            # cold start -> random
            return random.choice(MOVES), {"reason":"cold_start","votes":votes,"preds":preds}
        pred_opponent = int(max(range(3), key=lambda i: votes[i]))
        ai_move = COUNTER[IDX_TO_MOVE[pred_opponent]]
        return ai_move, {"reason":"ensemble", "votes":votes, "preds":preds}

    def inform_result(self, user_move_str: str, predicted_each: Dict[str, Optional[int]]):
        """Inform AI about actual opponent move: update predictors and their accuracy stats"""
        user_idx = MOVE_TO_IDX[user_move_str]
        # update rule predictors
        self.update_rule_predictors(user_idx)
        # update stats for each predictor
        for name, pred in predicted_each.items():
            if pred is None:
                continue
            self.pred_stats[name]['total'] += 1
            if pred == user_idx:
                self.pred_stats[name]['correct'] += 1

# ---------- CLI / interactive ----------
def parse_args():
    p = argparse.ArgumentParser(description="Advanced Rock-Paper-Scissors AI (self-play + ensemble)")
    p.add_argument("--selfplay", type=int, default=0,
                   help="Number of simulated training steps for self-play trainer (set 0 to skip training). "
                        "If you want 10 million, pass 10000000. (Default 0)")
    p.add_argument("--history_len", type=int, default=8, help="History length in moves (default 8)")
    p.add_argument("--batch_size", type=int, default=4096, help="Training batch size for self-play (default 4096)")
    p.add_argument("--model_path", type=str, default="rps_predictor.pt", help="Path to save/load neural predictor")
    p.add_argument("--device", type=str, default=("cuda" if torch.cuda.is_available() else "cpu"),
                   help="Device for training/model (cuda or cpu).")
    return p.parse_args()

def user_input_to_move(s: str) -> Optional[str]:
    s = s.strip().lower()
    mapping = {
        "r":"rock", "rock":"rock", "سنگ":"rock",
        "p":"paper", "paper":"paper", "کاغذ":"paper",
        "c":"scissors", "s":"scissors", "قیچی":"scissors"
    }
    return mapping.get(s, None)

def interactive_loop(ai: EnsembleRPSAI, rounds: int=20):
    print("=== Advanced RPS AI interactive mode ===")
    print("Enter your move: rock/paper/scissors (or r/p/c). Type 'quit' to exit.")
    score = {"ai":0, "you":0, "draw":0}
    for r in range(1, rounds+1):
        print(f"\n--- Round {r}/{rounds} ---")
        ai_move, meta = ai.choose_move()
        # but we shouldn't reveal AI move until user enters move
        while True:
            s = input("Your move: ").strip()
            if s.lower() in ("quit","exit"):
                print("Exiting.")
                return
            user_move = user_input_to_move(s)
            if user_move is None:
                print("Invalid. Use rock/paper/scissors or r/p/c.")
            else:
                break
        # gather predicted_each to update stats
        predicted_each = ai.predict_each()
        # show ai move and result
        res = winner(ai_move, user_move)
        if res == 1:
            print(f"AI: {ai_move}  | You: {user_move}  --> AI wins!")
            score['ai'] += 1
        elif res == -1:
            print(f"AI: {ai_move}  | You: {user_move}  --> You win!")
            score['you'] += 1
        else:
            print(f"AI: {ai_move}  | You: {user_move}  --> Draw.")
            score['draw'] += 1
        ai.inform_result(user_move, predicted_each)
        # optionally show meta info
        if meta.get("reason") == "ensemble":
            print(" (AI used ensemble) votes:", meta.get("votes"))
        else:
            print(" (AI reason):", meta.get("reason"))
    print("\n=== Final score ===")
    print(score)
    total = rounds
    print(f"AI win rate: {score['ai']}/{total} ({score['ai']/total*100:.1f}%)")

def main():
    args = parse_args()
    cfg = TrainerConfig(history_len=args.history_len, batch_size=args.batch_size,
                        selfplay_steps=args.selfplay, model_path=args.model_path, device=args.device)
    # If selfplay > 0 and model does not exist, train
    if args.selfplay > 0:
        print("[main] Starting self-play training with steps:", args.selfplay)
        trainer = SelfPlayTrainer(cfg)
        trainer.train(total_steps=args.selfplay)
        print("[main] Training complete.")
    # Now create ensemble AI (loading model if present)
    ensemble = EnsembleRPSAI(history_len=args.history_len, model_path=args.model_path, device=args.device)
    # If no model and user asked for selfplay=0, we still have rule-based predictors only
    # Enter interactive loop
    try:
        rounds = int(input("Enter number of rounds to play (e.g. 50): ").strip())
    except Exception:
        print("Invalid number -> default 20 rounds.")
        rounds = 20
    interactive_loop(ensemble, rounds=rounds)

if __name__ == "__main__":
    main()

